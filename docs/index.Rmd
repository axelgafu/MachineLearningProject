---
title: "Human Activity Recognition Analysis"
author: "Axel Garcia"
date: "January 27th, 2017"
output: html_document
---

```{r setup, include=FALSE}
library(caret)
knitr::opts_chunk$set(echo = TRUE)
set.seed(1234)

#
# File locations.
#-------------------------------------------------------------------------------
DATA_RAW_TRAIN <- "data\\train.csv"
DATA_RAW_TEST <- "data\\test.csv"
J48_MODEL <- "model\\j48_train.rds"

#
# Load Data.
#-------------------------------------------------------------------------------
if(!file.exists(DATA_RAW_TRAIN))
{
    download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", DATA_RAW_TRAIN)
}

if(!file.exists(DATA_RAW_TEST))
{
    download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", DATA_RAW_TEST)
}

train <- read.csv( DATA_RAW_TRAIN )
```

# Executive Summary
C4.5 algorithm is recommended to classify the data processed in this analysis since by using it a classification model can be trained in a reasonable time and the generalization error is relatively low.

# Overview
Data set under analysis has the lectures of several accelerometers used to conduct an experiment intended for movement classification: sitting, sitting down, standing, standing up and walking [1]. The provided data set [2] has `r length(train)` domains/variables in total. Presumably, there are raw accelerator measurements for each one of the x, y and z axis, as well as for other devices. The data set has the average, standard deviation, variance, kurtosis and skewness which certainly are correlated with some of the other variables.

The data set is composed of `r nrow(train)` records from which only `r sum(complete.cases(train)==TRUE)` are complete cases; i.e. records where all variables have a defined value. The data set has a variable with consecutive integers which seem to be the record ID.

Previous findings were used for data cleaning and the refined data set was processed with a classification trees algorithms. Sections below offer additional details.


## Data Cleaning
Missing values are useless for classification since do not help to explain the data variability. For the same reason, co-related variables are useless in this analysis. Those kinds of variables were removed from the training set. Besides them, the variable with the apparent sample ID was removed due to the variability of that variable is irrelevant to the classification.

Data cleaning has been done by using the following set of functions:

```{r util_functions}
#
# Returns true if the column name passed as parameter corresponds to an irrelevant variable name.
#
isInRemoveSet <- function(x)
{
    grepl( "*kurtosis*|*skewness*|*stddev*|var_*|avg_*|amplitude_*|X|max_*|min_*", x)
}

#
# Returns a reduced version of parameter "x" based on the reduction function 
#
reduceDimensions <- function(x, reduceDimFunction)
{
    x_colnames <- colnames(x)
    x_reducedColNames <- x_colnames[!reduceDimFunction(x_colnames)]

    x[, x_reducedColNames]
}
```

## Classification Model Generation
Train set was divided between training set and validation set in order to prevent overfitting and to use the train set as support to estimate the generalization error.
```{r build_sets, cache=TRUE}
# Read raw train data set and reduce dimention
train <- read.csv( DATA_RAW_TRAIN )
train_reduced <- reduceDimensions(train, isInRemoveSet)

# Divide between training and validation set.
inTrain <- createDataPartition(y=train_reduced$classe, p=0.3, list=FALSE)
training_set <- train_reduced[inTrain, ]
validate_set <- train_reduced[-inTrain, ]
```

Ross Quinlan’s [3] C4.5 decision tree has been used for this classification problem. It uses the information gain metric to identify the variables with higher relevance for the classification and uses them to create the decision tree. Cross-validation train control is being used to measure the training error in 10 folds. 

```{r train, message=FALSE}
train_control <- trainControl(method="cv", number=10)
j48_train <- train(classe~., method="J48", data=training_set, trControl=train_control)

summary(j48_train)
```

Trained model is now tested against the validation set in order to estimate the generalization error.
```{r eval,cache=TRUE}
# Evaluation and Confusion Matrix
j48_validation <- predict(j48_train, validate_set[,-c(59)])
cm <- confusionMatrix(j48_validation, validate_set$classe)
cm
```

## Generalization Error
### Evaluation Function
Classifier outcome is the predicted category for the actual sample under evaluation. The correctness of that outcome can be evaluated by following a binary criteria: correctly classified or incorrectly classified. The given training set contains the true classification of each sample in it. Thus it is possible to measure the generalization error (i.e. out sample error) by allocating a portion of the training set for testing the classifier; in this case the allocation portion for that purpose is 70% of the samples in the training set.

An evaluation function is used to collect the error data. That function takes as input the predicted value and the true value. The evaluation function used for the generalization error will evaluate to 0 when the predicted category is equal to the true category and 1 if it is not. In that way the error will be translated to a numeric value that can be used for the generalization error estimation.

### Generalization Error Measurement
Since the evaluation function will count how many incorrectly classified instances the classifier produces. Averaging that count will show the portion of error that corresponds to each prediction; giving a descriptive number since the measurement will consider the context of the metric(i.e. sample size). Mean Absolute Deviation(MAD) is an the error measurement equation which is close to the intended measurement criteria. MAD is not used to be expressed as percentage but as a real number [4,5]:
```{r generalization error,cache=TRUE}
# Evaluation function: Evaluates predicted value (x) vs true value (y)
#    0 Correctly classified.
#    1 Incorrectly classified.
evaluate <- function(x, y)
{
    x != y
}

# Mean Absolute Deviation: Sum of the errors divided by the sample size.
j48_validation_mad <- sum(evaluate(validate_set$classe, j48_validation))/nrow(validate_set)
round(j48_validation_mad, digits=4)
```

## Conclusion
Objective is to correctly infer each one of the records category. The approach followed in this analysis was to considering this as a classification problem. That decision has led this analysis to a classification model which has a generalization error of `r round(j48_validation_mad, 4)` and an Accuracy of `r round(cm$overall['Accuracy']*100, 2)`% using 30% of the train samples for the actual model training and 70% as validation set. Those are considered satisfactory results.

The same decision trees algorithm than [1] was used, however, different results were obtained. In the referred work the authors do not discuss how the data cleaning step was performed. It is assumed differences in that step caused the difference between that work and the analysis results in this document.

Classification trees seem to be sufficient to achieve the objective of this analysis.



## References
[1] Ugulino, Wallace, et al. "Wearable computing: Accelerometers’ data classification of body postures and movements." Advances in Artificial Intelligence-SBIA 2012. Springer Berlin Heidelberg, 2012. 52-61., [http://groupware.les.inf.puc-rio.br/work.jsf?p1=10335](http://groupware.les.inf.puc-rio.br/work.jsf?p1=10335)

[2] Wallace U., Eduardo V., Hugo F., "Human Activity Recognition", 2017/01/27, online:  [http://groupware.les.inf.puc-rio.br/har](http://groupware.les.inf.puc-rio.br/har)

[3] Salzberg, Steven L. "C4. 5: Programs for machine learning by J. Ross Quinlan. Morgan Kaufmann publishers, inc., 1993." Machine Learning 16.3 (1994): 235-240.

[4] What's the bottom line? How to compare models, duke.edu, 2017/01/29, online:  [https://people.duke.edu/~rnau/compare.htm](https://people.duke.edu/~rnau/compare.htm)

[5] Measuring Forecast Accuracy: Approaches to Forecasting : A Tutorial, NC State Univeristy, [https://scm.ncsu.edu/scm-articles/article/measuring-forecast-accuracy-approaches-to-forecasting-a-tutorial](https://scm.ncsu.edu/scm-articles/article/measuring-forecast-accuracy-approaches-to-forecasting-a-tutorial)


